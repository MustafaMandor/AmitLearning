{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf32987e-f375-4b95-9e11-3f3bc65a64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import re \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Dense,Dropout,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feb98b1-adcb-4f8c-8010-c77134bb6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "data=pd.read_table('raw_in_domain_train.tsv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055024e2-8b25-459e-ab86-b37fc292ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Poseidon appears to own a dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Digitize is my happiest memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It is easy to slay the Gorgon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I had the strangest feeling that I knew you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What all did you get for Christmas?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8551 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1    2                                                  3\n",
       "0     gj04  1  NaN  Our friends won't buy this analysis, let alone...\n",
       "1     gj04  1  NaN  One more pseudo generalization and I'm giving up.\n",
       "2     gj04  1  NaN   One more pseudo generalization or I'm giving up.\n",
       "3     gj04  1  NaN     The more we study verbs, the crazier they get.\n",
       "4     gj04  1  NaN          Day by day the facts are getting murkier.\n",
       "...    ... ..  ...                                                ...\n",
       "8546  ad03  0    *                   Poseidon appears to own a dragon\n",
       "8547  ad03  0    *                     Digitize is my happiest memory\n",
       "8548  ad03  1  NaN                     It is easy to slay the Gorgon.\n",
       "8549  ad03  1  NaN       I had the strangest feeling that I knew you.\n",
       "8550  ad03  1  NaN                What all did you get for Christmas?\n",
       "\n",
       "[8551 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098f7161-8397-4d7d-84cb-eda96279295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transform the texts column into a list\n",
    "texts=data[3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148c3023-9270-48dc-bc08-f99c86002195",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function for cleaning texts\n",
    "def clean_text(text): \n",
    "    text=re.sub(\"(\\?|!)+\",\" \",text) #remvoe ? and !\n",
    "    text=re.sub(\"\\s\\d+\\s\",\"\",text) # remove digits \n",
    "    text=re.sub(\"(\\.|\\,)+\",\"\",text) #remove . and ,\n",
    "    text=re.sub(\"^\\s+\",\"\",text) #remove space in the begining\n",
    "    text=re.sub(\"\\s+$\",\"\",text) #remove space in the end\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a710330-aca8-41e8-b071-09877536572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function for processing texts\n",
    "def process_sentence(texts):\n",
    "    clean_texts=[]\n",
    "    for text in texts:\n",
    "        text=clean_text(text) #Cleaning the text\n",
    "        clean_texts.append(text.lower()) #Lowercasing the texts\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58099cc6-9558-42ed-b62c-cd42a5f43dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"our friends won't buy this analysis let alone the next one we propose\",\n",
       " \"one more pseudo generalization and i'm giving up\",\n",
       " \"one more pseudo generalization or i'm giving up\",\n",
       " 'the more we study verbs the crazier they get',\n",
       " 'day by day the facts are getting murkier']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_texts=process_sentence(texts)\n",
    "clean_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b44e1f-9b2b-4458-a37c-07bce4ee9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization Process and making sequence of tokens for each text\n",
    "max_words = 6000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words) # Size of dictionary containig each word and its index\n",
    "tokenizer.fit_on_texts(clean_texts) # Tokenize each word\n",
    "sequences = tokenizer.texts_to_sequences(clean_texts) # convert the texts into tokenized sequences\n",
    "vocab_size = len(tokenizer.word_index) # Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfac6652-caed-4158-b468-def360c0d181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[221, 271, 253, 164, 28, 698, 753, 1288, 1, 699, 79, 29, 3571],\n",
       "  [79, 25, 2624, 2625, 9, 144, 1478, 49],\n",
       "  [79, 25, 2624, 2625, 138, 144, 1478, 49],\n",
       "  [1, 25, 29, 448, 3572, 1, 1289, 30, 210],\n",
       "  [427, 34, 427, 1, 3573, 45, 902, 3574]],\n",
       " 5524)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5],vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce18457-fc9f-46f2-9066-fb2823889dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(sequences,key=len)) #Max is a built in function to find the longest text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1647646c-0e0a-4cc6-a5f3-c38d7f6b4c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,  221,  271,  253,\n",
       "         164,   28,  698,  753, 1288,    1,  699,   79,   29, 3571],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,   79,   25, 2624, 2625,    9,  144, 1478,   49],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,   79,   25, 2624, 2625,  138,  144, 1478,   49],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1,   25,   29,  448, 3572,    1, 1289,   30,  210],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  427,   34,  427,    1, 3573,   45,  902, 3574]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Padding of the encoded texts\n",
    "padded_texts=pad_sequences(sequences,maxlen=43,padding='pre')\n",
    "padded_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b54d5f1f-35af-44c6-8d83-4ee063dc2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the input vector X and the label vector Y\n",
    "import keras.utils as ku\n",
    "label = ku.to_categorical(padded_texts[:,-1], num_classes=vocab_size)\n",
    "y=label\n",
    "x=padded_texts[:,:-1]\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "627f766e-5111-40f9-9b91-a0dd6285e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 42, 10)            55250     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 40)                8160      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               4100      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 5524)              557924    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 625,434\n",
      "Trainable params: 625,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size+1,output_dim=10,input_length=42))\n",
    "model.add(LSTM(40,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(vocab_size,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a92cbbe-d773-4fc3-9e0e-fcb130bcbbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "134/134 [==============================] - 4s 16ms/step - loss: 8.3936 - accuracy: 0.0074\n",
      "Epoch 2/100\n",
      "134/134 [==============================] - 2s 16ms/step - loss: 7.3055 - accuracy: 0.0085\n",
      "Epoch 3/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 7.1409 - accuracy: 0.0110\n",
      "Epoch 4/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 7.0816 - accuracy: 0.0120\n",
      "Epoch 5/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 7.0169 - accuracy: 0.0095\n",
      "Epoch 6/100\n",
      "134/134 [==============================] - 2s 16ms/step - loss: 6.9182 - accuracy: 0.0140\n",
      "Epoch 7/100\n",
      "134/134 [==============================] - 2s 16ms/step - loss: 6.7867 - accuracy: 0.0153\n",
      "Epoch 8/100\n",
      "134/134 [==============================] - 2s 16ms/step - loss: 6.5733 - accuracy: 0.0182\n",
      "Epoch 9/100\n",
      "134/134 [==============================] - 2s 16ms/step - loss: 6.3218 - accuracy: 0.0219\n",
      "Epoch 10/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 6.0821 - accuracy: 0.0296\n",
      "Epoch 11/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 5.8393 - accuracy: 0.0387\n",
      "Epoch 12/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 5.5889 - accuracy: 0.0505\n",
      "Epoch 13/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 5.3512 - accuracy: 0.0620\n",
      "Epoch 14/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 5.1112 - accuracy: 0.0765\n",
      "Epoch 15/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 4.8655 - accuracy: 0.0943\n",
      "Epoch 16/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 4.6102 - accuracy: 0.1158\n",
      "Epoch 17/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 4.3753 - accuracy: 0.1368\n",
      "Epoch 18/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 4.1411 - accuracy: 0.1629\n",
      "Epoch 19/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 3.9093 - accuracy: 0.1850\n",
      "Epoch 20/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 3.6969 - accuracy: 0.2109\n",
      "Epoch 21/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 3.5076 - accuracy: 0.2392\n",
      "Epoch 22/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 3.3159 - accuracy: 0.2604\n",
      "Epoch 23/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 3.1946 - accuracy: 0.2906\n",
      "Epoch 24/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 2.9497 - accuracy: 0.3221\n",
      "Epoch 25/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 2.8026 - accuracy: 0.3540\n",
      "Epoch 26/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 2.6670 - accuracy: 0.3767\n",
      "Epoch 27/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 2.5262 - accuracy: 0.4069\n",
      "Epoch 28/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 2.3353 - accuracy: 0.4443\n",
      "Epoch 29/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 2.1699 - accuracy: 0.4853\n",
      "Epoch 30/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 2.0854 - accuracy: 0.4984\n",
      "Epoch 31/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 2.0317 - accuracy: 0.5115\n",
      "Epoch 32/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.8492 - accuracy: 0.5436\n",
      "Epoch 33/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 1.7787 - accuracy: 0.5673\n",
      "Epoch 34/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.6534 - accuracy: 0.5943\n",
      "Epoch 35/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.5725 - accuracy: 0.6095\n",
      "Epoch 36/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.5120 - accuracy: 0.6224\n",
      "Epoch 37/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.4680 - accuracy: 0.6316\n",
      "Epoch 38/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 1.3626 - accuracy: 0.6569\n",
      "Epoch 39/100\n",
      "134/134 [==============================] - 2s 19ms/step - loss: 1.3005 - accuracy: 0.6726\n",
      "Epoch 40/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.2298 - accuracy: 0.6890\n",
      "Epoch 41/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.1611 - accuracy: 0.7044\n",
      "Epoch 42/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.1080 - accuracy: 0.7148\n",
      "Epoch 43/100\n",
      "134/134 [==============================] - 2s 19ms/step - loss: 1.0444 - accuracy: 0.7311\n",
      "Epoch 44/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 1.0407 - accuracy: 0.7292\n",
      "Epoch 45/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.9512 - accuracy: 0.7525\n",
      "Epoch 46/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.9535 - accuracy: 0.7465\n",
      "Epoch 47/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.8868 - accuracy: 0.7642\n",
      "Epoch 48/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.8479 - accuracy: 0.7757\n",
      "Epoch 49/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.8239 - accuracy: 0.7780\n",
      "Epoch 50/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.9075 - accuracy: 0.7597\n",
      "Epoch 51/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.7872 - accuracy: 0.7863\n",
      "Epoch 52/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.7506 - accuracy: 0.7982\n",
      "Epoch 53/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.7366 - accuracy: 0.8019\n",
      "Epoch 54/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.6784 - accuracy: 0.8155\n",
      "Epoch 55/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.6208 - accuracy: 0.8341\n",
      "Epoch 56/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.6560 - accuracy: 0.8232\n",
      "Epoch 57/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.5831 - accuracy: 0.8390\n",
      "Epoch 58/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.5807 - accuracy: 0.8417\n",
      "Epoch 59/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.5724 - accuracy: 0.8394\n",
      "Epoch 60/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5612 - accuracy: 0.8398\n",
      "Epoch 61/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.6276 - accuracy: 0.8265\n",
      "Epoch 62/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5706 - accuracy: 0.8374\n",
      "Epoch 63/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.5398 - accuracy: 0.8505\n",
      "Epoch 64/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.5683 - accuracy: 0.8421\n",
      "Epoch 65/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.4746 - accuracy: 0.8676\n",
      "Epoch 66/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.4744 - accuracy: 0.8666\n",
      "Epoch 67/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4387 - accuracy: 0.8773\n",
      "Epoch 68/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4257 - accuracy: 0.8843\n",
      "Epoch 69/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3950 - accuracy: 0.8908\n",
      "Epoch 70/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.5637 - accuracy: 0.8431\n",
      "Epoch 71/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.4489 - accuracy: 0.8689\n",
      "Epoch 72/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.4243 - accuracy: 0.8783\n",
      "Epoch 73/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3982 - accuracy: 0.8861\n",
      "Epoch 74/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3664 - accuracy: 0.8937\n",
      "Epoch 75/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3365 - accuracy: 0.9046\n",
      "Epoch 76/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3961 - accuracy: 0.8867\n",
      "Epoch 77/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.4462 - accuracy: 0.8729\n",
      "Epoch 78/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.5244 - accuracy: 0.8505\n",
      "Epoch 79/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3348 - accuracy: 0.9070\n",
      "Epoch 80/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3046 - accuracy: 0.9137\n",
      "Epoch 81/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.2907 - accuracy: 0.9166\n",
      "Epoch 82/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.2929 - accuracy: 0.9163\n",
      "Epoch 83/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3148 - accuracy: 0.9057\n",
      "Epoch 84/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.2733 - accuracy: 0.9205\n",
      "Epoch 85/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.3668 - accuracy: 0.8916\n",
      "Epoch 86/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3602 - accuracy: 0.8950\n",
      "Epoch 87/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3472 - accuracy: 0.8945\n",
      "Epoch 88/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.3301 - accuracy: 0.9007\n",
      "Epoch 89/100\n",
      "134/134 [==============================] - 3s 25ms/step - loss: 0.2869 - accuracy: 0.9138\n",
      "Epoch 90/100\n",
      "134/134 [==============================] - 4s 27ms/step - loss: 0.2880 - accuracy: 0.9118\n",
      "Epoch 91/100\n",
      "134/134 [==============================] - 4s 26ms/step - loss: 0.2624 - accuracy: 0.9234\n",
      "Epoch 92/100\n",
      "134/134 [==============================] - 3s 23ms/step - loss: 0.2558 - accuracy: 0.9249\n",
      "Epoch 93/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.2957 - accuracy: 0.9154\n",
      "Epoch 94/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.2793 - accuracy: 0.9178\n",
      "Epoch 95/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.2876 - accuracy: 0.9172\n",
      "Epoch 96/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.2741 - accuracy: 0.9180\n",
      "Epoch 97/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.2550 - accuracy: 0.9219\n",
      "Epoch 98/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.2465 - accuracy: 0.9282\n",
      "Epoch 99/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.2866 - accuracy: 0.9151\n",
      "Epoch 100/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.2484 - accuracy: 0.9273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6bfef1460>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(x,y,epochs=100,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d6a1e31-0023-4950-a5e2-583bc60a7cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 5ms/step - loss: 0.1807 - accuracy: 0.9533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18072830140590668, 0.9532710313796997]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Evaluation\n",
    "model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "14a8f82c-a1d1-4df8-a4c8-e521e46267fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 1s 6ms/step - loss: 0.2198 - accuracy: 0.9367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21980755031108856, 0.9367121458053589]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation for overfitting\n",
    "model.evaluate(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "703b2c10-8140-4b71-8826-cedcd3df4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to generate text including the text sent,number of words generated, model\n",
    "def generate_text(text, words_generated, model):\n",
    "    for i in range(words_generated):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]  #Tokenize the new text\n",
    "        token_list = pad_sequences([token_list], maxlen=42, padding='pre') #padding the new token\n",
    "        predicted = model.predict(token_list) #Prediction\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted.argmax()+1: #get the index of the max output and getting the word from the tokenizer\n",
    "                output_word = word\n",
    "                break\n",
    "        text += \" \"+output_word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bbf24ea8-10f9-4eaa-bef3-d5aa74a2ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i dont remember ridden butter factors specialized'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('i dont remember',4,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd74a7-1d37-40de-939b-eda5253d6237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf32987e-f375-4b95-9e11-3f3bc65a64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Dense,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feb98b1-adcb-4f8c-8010-c77134bb6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "data=pd.read_table('raw_in_domain_train.tsv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055024e2-8b25-459e-ab86-b37fc292ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Poseidon appears to own a dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Digitize is my happiest memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It is easy to slay the Gorgon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I had the strangest feeling that I knew you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What all did you get for Christmas?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8551 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1    2                                                  3\n",
       "0     gj04  1  NaN  Our friends won't buy this analysis, let alone...\n",
       "1     gj04  1  NaN  One more pseudo generalization and I'm giving up.\n",
       "2     gj04  1  NaN   One more pseudo generalization or I'm giving up.\n",
       "3     gj04  1  NaN     The more we study verbs, the crazier they get.\n",
       "4     gj04  1  NaN          Day by day the facts are getting murkier.\n",
       "...    ... ..  ...                                                ...\n",
       "8546  ad03  0    *                   Poseidon appears to own a dragon\n",
       "8547  ad03  0    *                     Digitize is my happiest memory\n",
       "8548  ad03  1  NaN                     It is easy to slay the Gorgon.\n",
       "8549  ad03  1  NaN       I had the strangest feeling that I knew you.\n",
       "8550  ad03  1  NaN                What all did you get for Christmas?\n",
       "\n",
       "[8551 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098f7161-8397-4d7d-84cb-eda96279295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transform the texts column into a list\n",
    "texts=data[3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148c3023-9270-48dc-bc08-f99c86002195",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function for cleaning texts\n",
    "def clean_text(text): \n",
    "    text=re.sub(\"(\\?|!)+\",\" \",text) #remvoe ? and !\n",
    "    text=re.sub(\"\\s\\d+\\s\",\"\",text) # remove digits \n",
    "    text=re.sub(\"(\\.|\\,)+\",\"\",text) #remove . and ,\n",
    "    text=re.sub(\"^\\s+\",\"\",text) #remove space in the begining\n",
    "    text=re.sub(\"\\s+$\",\"\",text) #remove space in the end\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a710330-aca8-41e8-b071-09877536572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function for processing texts\n",
    "def process_sentence(texts):\n",
    "    clean_texts=[]\n",
    "    for text in texts:\n",
    "        text=clean_text(text) #Cleaning the text\n",
    "        clean_texts.append(text.lower()) #Lowercasing the texts\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58099cc6-9558-42ed-b62c-cd42a5f43dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"our friends won't buy this analysis let alone the next one we propose\",\n",
       " \"one more pseudo generalization and i'm giving up\",\n",
       " \"one more pseudo generalization or i'm giving up\",\n",
       " 'the more we study verbs the crazier they get',\n",
       " 'day by day the facts are getting murkier']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_texts=process_sentence(texts)\n",
    "clean_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b44e1f-9b2b-4458-a37c-07bce4ee9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization Process and making sequence of tokens for each text\n",
    "max_words = 6000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words) # Size of dictionary containig each word and its index\n",
    "tokenizer.fit_on_texts(clean_texts) # Tokenize each word\n",
    "sequences = tokenizer.texts_to_sequences(clean_texts) # convert the texts into tokenized sequences\n",
    "vocab_size = len(tokenizer.word_index) # Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfac6652-caed-4158-b468-def360c0d181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[221, 271, 253, 164, 28, 698, 753, 1288, 1, 699, 79, 29, 3571],\n",
       "  [79, 25, 2624, 2625, 9, 144, 1478, 49],\n",
       "  [79, 25, 2624, 2625, 138, 144, 1478, 49],\n",
       "  [1, 25, 29, 448, 3572, 1, 1289, 30, 210],\n",
       "  [427, 34, 427, 1, 3573, 45, 902, 3574]],\n",
       " 5524)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5],vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce18457-fc9f-46f2-9066-fb2823889dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(sequences,key=len)) #Max is a built in function to find the longest text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1647646c-0e0a-4cc6-a5f3-c38d7f6b4c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,  221,  271,  253,\n",
       "         164,   28,  698,  753, 1288,    1,  699,   79,   29, 3571],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,   79,   25, 2624, 2625,    9,  144, 1478,   49],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,   79,   25, 2624, 2625,  138,  144, 1478,   49],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1,   25,   29,  448, 3572,    1, 1289,   30,  210],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  427,   34,  427,    1, 3573,   45,  902, 3574]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Padding of the encoded texts\n",
    "padded_texts=pad_sequences(sequences,maxlen=43,padding='pre')\n",
    "padded_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54d5f1f-35af-44c6-8d83-4ee063dc2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the input vector X and the label vector Y\n",
    "import keras.utils as ku\n",
    "label = ku.to_categorical(padded_texts[:,-1], num_classes=vocab_size)\n",
    "y=label\n",
    "x=padded_texts[:,:-1]\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627f766e-5111-40f9-9b91-a0dd6285e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 42, 10)            55250     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 40)                8160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               4100      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5524)              557924    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 625,434\n",
      "Trainable params: 625,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size+1,output_dim=10,input_length=42))\n",
    "model.add(LSTM(40,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(vocab_size,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a92cbbe-d773-4fc3-9e0e-fcb130bcbbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "134/134 [==============================] - 4s 17ms/step - loss: 8.0728 - accuracy: 0.0104\n",
      "Epoch 2/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 7.2124 - accuracy: 0.0112\n",
      "Epoch 3/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 7.1123 - accuracy: 0.0118\n",
      "Epoch 4/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 7.0494 - accuracy: 0.0123\n",
      "Epoch 5/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 6.9824 - accuracy: 0.0108\n",
      "Epoch 6/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 6.9006 - accuracy: 0.0098\n",
      "Epoch 7/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 6.7690 - accuracy: 0.0125\n",
      "Epoch 8/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 6.6108 - accuracy: 0.0163\n",
      "Epoch 9/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 6.4472 - accuracy: 0.0196\n",
      "Epoch 10/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 6.2791 - accuracy: 0.0235\n",
      "Epoch 11/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 6.0875 - accuracy: 0.0271\n",
      "Epoch 12/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 5.8870 - accuracy: 0.0346\n",
      "Epoch 13/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 5.6760 - accuracy: 0.0457\n",
      "Epoch 14/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 5.4797 - accuracy: 0.0517\n",
      "Epoch 15/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 5.2935 - accuracy: 0.0647\n",
      "Epoch 16/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 5.0715 - accuracy: 0.0767\n",
      "Epoch 17/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 4.8480 - accuracy: 0.0917\n",
      "Epoch 18/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 4.6337 - accuracy: 0.1075\n",
      "Epoch 19/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 4.4118 - accuracy: 0.1237\n",
      "Epoch 20/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 4.1680 - accuracy: 0.1468\n",
      "Epoch 21/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 3.9873 - accuracy: 0.1661\n",
      "Epoch 22/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 3.7694 - accuracy: 0.1932\n",
      "Epoch 23/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 3.5684 - accuracy: 0.2159\n",
      "Epoch 24/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 3.4116 - accuracy: 0.2404\n",
      "Epoch 25/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 3.2188 - accuracy: 0.2733\n",
      "Epoch 26/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 3.0125 - accuracy: 0.2960\n",
      "Epoch 27/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 2.8653 - accuracy: 0.3281\n",
      "Epoch 28/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 2.7890 - accuracy: 0.3407\n",
      "Epoch 29/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 2.6081 - accuracy: 0.3746\n",
      "Epoch 30/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 2.4504 - accuracy: 0.4031\n",
      "Epoch 31/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 2.3210 - accuracy: 0.4284\n",
      "Epoch 32/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 2.1758 - accuracy: 0.4599\n",
      "Epoch 33/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 2.0772 - accuracy: 0.4803\n",
      "Epoch 34/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 1.9524 - accuracy: 0.5072\n",
      "Epoch 35/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 1.8387 - accuracy: 0.5353\n",
      "Epoch 36/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.7791 - accuracy: 0.5458\n",
      "Epoch 37/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.6781 - accuracy: 0.5685\n",
      "Epoch 38/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 1.5921 - accuracy: 0.5844\n",
      "Epoch 39/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 1.5310 - accuracy: 0.5995\n",
      "Epoch 40/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 1.4616 - accuracy: 0.6149\n",
      "Epoch 41/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 1.3804 - accuracy: 0.6372\n",
      "Epoch 42/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.3365 - accuracy: 0.6434\n",
      "Epoch 43/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.2414 - accuracy: 0.6674\n",
      "Epoch 44/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 1.1724 - accuracy: 0.6869\n",
      "Epoch 45/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 1.1405 - accuracy: 0.6945\n",
      "Epoch 46/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 1.0638 - accuracy: 0.7092\n",
      "Epoch 47/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 1.0185 - accuracy: 0.7193\n",
      "Epoch 48/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.9901 - accuracy: 0.7289\n",
      "Epoch 49/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.9889 - accuracy: 0.7241\n",
      "Epoch 50/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.8945 - accuracy: 0.7539\n",
      "Epoch 51/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.8560 - accuracy: 0.7614\n",
      "Epoch 52/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.8304 - accuracy: 0.7649\n",
      "Epoch 53/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.9021 - accuracy: 0.7455\n",
      "Epoch 54/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.7961 - accuracy: 0.7770\n",
      "Epoch 55/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.7240 - accuracy: 0.7949\n",
      "Epoch 56/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.6923 - accuracy: 0.8034\n",
      "Epoch 57/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.7427 - accuracy: 0.7873\n",
      "Epoch 58/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.6737 - accuracy: 0.8061\n",
      "Epoch 59/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.6435 - accuracy: 0.8145\n",
      "Epoch 60/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.6048 - accuracy: 0.8272\n",
      "Epoch 61/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.5974 - accuracy: 0.8321\n",
      "Epoch 62/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.5661 - accuracy: 0.8349\n",
      "Epoch 63/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.5591 - accuracy: 0.8390\n",
      "Epoch 64/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.5784 - accuracy: 0.8302\n",
      "Epoch 65/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.6060 - accuracy: 0.8215\n",
      "Epoch 66/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.5379 - accuracy: 0.8411\n",
      "Epoch 67/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.4997 - accuracy: 0.8522\n",
      "Epoch 68/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.5189 - accuracy: 0.8496\n",
      "Epoch 69/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 0.4879 - accuracy: 0.8587\n",
      "Epoch 70/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.4708 - accuracy: 0.8649\n",
      "Epoch 71/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.4431 - accuracy: 0.8709\n",
      "Epoch 72/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.4240 - accuracy: 0.8740\n",
      "Epoch 73/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.4312 - accuracy: 0.8703\n",
      "Epoch 74/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.4783 - accuracy: 0.8592\n",
      "Epoch 75/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.4423 - accuracy: 0.8696\n",
      "Epoch 76/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.4327 - accuracy: 0.8735\n",
      "Epoch 77/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3985 - accuracy: 0.8828\n",
      "Epoch 78/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3816 - accuracy: 0.8894\n",
      "Epoch 79/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3547 - accuracy: 0.8963\n",
      "Epoch 80/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.4902 - accuracy: 0.8515\n",
      "Epoch 81/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.3774 - accuracy: 0.8821\n",
      "Epoch 82/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3358 - accuracy: 0.9018\n",
      "Epoch 83/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3119 - accuracy: 0.9107\n",
      "Epoch 84/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3211 - accuracy: 0.9039\n",
      "Epoch 85/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3274 - accuracy: 0.9009\n",
      "Epoch 86/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 0.3129 - accuracy: 0.9085\n",
      "Epoch 87/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 0.2777 - accuracy: 0.9190\n",
      "Epoch 88/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3349 - accuracy: 0.9028\n",
      "Epoch 89/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.3471 - accuracy: 0.8970\n",
      "Epoch 90/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3812 - accuracy: 0.8847\n",
      "Epoch 91/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.3401 - accuracy: 0.8946\n",
      "Epoch 92/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.2966 - accuracy: 0.9144\n",
      "Epoch 93/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 0.2777 - accuracy: 0.9178\n",
      "Epoch 94/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.2944 - accuracy: 0.9124\n",
      "Epoch 95/100\n",
      "134/134 [==============================] - 3s 20ms/step - loss: 0.2471 - accuracy: 0.9249\n",
      "Epoch 96/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.2621 - accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.2772 - accuracy: 0.9181\n",
      "Epoch 98/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.2963 - accuracy: 0.9097\n",
      "Epoch 99/100\n",
      "134/134 [==============================] - 3s 21ms/step - loss: 0.3323 - accuracy: 0.8971\n",
      "Epoch 100/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3115 - accuracy: 0.9053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2357ce6d520>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(train_x,train_y,epochs=100,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d6a1e31-0023-4950-a5e2-583bc60a7cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 5ms/step - loss: 0.2439 - accuracy: 0.9217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24387529492378235, 0.9217289686203003]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Evaluation\n",
    "model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a8f82c-a1d1-4df8-a4c8-e521e46267fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 1s 5ms/step - loss: 0.2352 - accuracy: 0.9272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23521637916564941, 0.9272254705429077]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation for overfitting\n",
    "model.evaluate(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "703b2c10-8140-4b71-8826-cedcd3df4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to generate text including the text sent,number of words generated, model\n",
    "def generate_text(text, words_generated, model):\n",
    "    for i in range(words_generated):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]  #Tokenize the new text\n",
    "        token_list = pad_sequences([token_list], maxlen=42, padding='pre') #padding the new token\n",
    "        predicted = model.predict(token_list) #Prediction\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted.argmax(): #get the index of the max output and getting the word from the tokenizer\n",
    "                output_word = word\n",
    "                break\n",
    "        text += \" \"+output_word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf24ea8-10f9-4eaa-bef3-d5aa74a2ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i dont remember sugar fool helen medicine'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text Generation\n",
    "generate_text('i dont remember',4,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edc322-a8fc-4580-9fd4-3e782928ec3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
